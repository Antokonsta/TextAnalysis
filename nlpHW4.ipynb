{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем нужен слой эмбединга?\n",
    "\n",
    "Данный слой связан с парадигмой Word2Vec (Векторное представление слов). Как различить слова  \"идти\" и \"шагать\"?  Для компьютера это разные строки, но по векторному преставлению они будут иметь близкие векторы.\n",
    "\n",
    "Этот слой необходим, потому что многие алгоритмы машинного обучения (включая глубокие сети) требуют, чтобы их входные данные были векторами непрерывных значений; Они не будут работать со строками простого текста. Поэтому данный слой используется для сопоставления слов или фраз из словаря с соответствующим вектором действительных чисел. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чем отличается RNN от LSTM?\n",
    "\n",
    "Все RNN имеют обратные связи в рекуррентном слое. Это позволяет им сохранять информацию в памяти с течением времени. Но может быть сложно обучить стандартные RNN для решения проблем, требующих изучения долгосрочных временных зависимостей. Это связано с тем, что градиент функции потерь экспоненциально убывает со временем (проблема исчезающего градиента). Сети LSTM - это тип RNN, который использует специальные модули в дополнение к стандартным. Блоки LSTM включают в себя «ячейку памяти», которая может хранить информацию в памяти в течение длительных периодов времени. Набор \"вентилей\" используется для контроля того, когда информация поступает в память, когда она выводится, и когда она забывается. Эта архитектура позволяет лучше управлять потоком градиента и обеспечивают лучшее сохранение «долгосрочных зависимостей». "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чем RNN лучше чем свёрточная сеть?\n",
    "\n",
    "Сверточные нейронные сети (CNN) предназначены для обработки изображений. Рекуррентные нейронные сети (RNN) предназначены для обработки речевого сигнала или текста.\n",
    "\n",
    "CNN принимают вход фиксированного размера и генерируют выходы фиксированного размера. RNN, с другой стороны, может обрабатывать произвольные длины ввода / вывода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/Anton/Desktop/Учеба/Тексты/hw3/avito_train_1kk.tsv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.22822210732570425\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print (\"Blocked ratio\",df.is_blocked.mean())\n",
    "print (\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Балансировка выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Балансировка выборки таким образом, чтобы  0.49 < blocked ratio < 0.51 и размер df <= 560k\n",
    "df = df[df.is_blocked == 1].append(df[df.is_blocked == 0].sample(285000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.491067793341381\n",
      "Count: 559996\n"
     ]
    }
   ],
   "source": [
    "print (\"Blocked ratio\",df.is_blocked.mean())\n",
    "print (\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "#Удовлетворяем требованиям\n",
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print (\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаляем редкие токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "tokens = [token for token in token_counts.keys() if token_counts[token] >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 106642\n"
     ]
    }
   ],
   "source": [
    "print (\"# Tokens:\",len(token_to_id))\n",
    "if len(token_to_id) < 30000:\n",
    "    print (\"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\")\n",
    "if len(token_to_id) > 1000000:\n",
    "    print (\"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Меняем тексты и ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = list(map(lambda token: token_to_id.get(token,0), tokens))[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (559996, 15)\n",
      "Поездки на таможню, печать в паспорте -> [    1     2     3 21589    74  8657     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [15548     0   363     0     0     0     0     0     0     0] ...\n",
      "Возьму суду под200 т. р -> [ 62  63   0 146 358   0   0   0   0   0] ...\n"
     ]
    }
   ],
   "source": [
    "# Например\n",
    "print (\"Размер матрицы:\",title_tokens.shape)\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print (title,'->', tokens[:10],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предварительная обработка некоторых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "\n",
    "categories = [{\"category\":category_name, \"subcategory\":subcategory_name}\\\n",
    "              for category_name, subcategory_name in data_cat_subcat]\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделение выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "splits = train_test_split(title_tokens,desc_tokens,df_non_text,target,test_size = 0.3)\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение предобработанных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading saved data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "save_prepared_data = False #save\n",
    "read_prepared_data = True #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print (\"Saving preprocessed data (may take up to 3 minutes)\")\n",
    "\n",
    "    import pickle\n",
    "    data_tuple = title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts\n",
    "    with open(\"preprocessed_data.pcl\",'wb') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'wb') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print (\"готово\")\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print (\"Reading saved data...\")\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'rb') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'rb') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print (\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/Theano/Theano/archive/master.zip\n",
      "  Downloading https://github.com/Theano/Theano/archive/master.zip (13.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.2MB 45kB/s eta 0:00:011   34% |███████████▏                    | 4.6MB 1.5MB/s eta 0:00:06\n",
      "\u001b[?25hCollecting numpy>=1.9.1 (from Theano==0.9.0)\n",
      "  Downloading numpy-1.12.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.4MB 134kB/s ta 0:00:011    86% |███████████████████████████▉    | 3.9MB 8.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy>=0.14 (from Theano==0.9.0)\n",
      "  Downloading scipy-0.19.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (16.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 16.2MB 35kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already up-to-date: six>=1.9.0 in /anaconda/lib/python3.6/site-packages (from Theano==0.9.0)\n",
      "Installing collected packages: numpy, scipy, Theano\n",
      "  Found existing installation: numpy 1.11.3\n",
      "    Uninstalling numpy-1.11.3:\n",
      "      Successfully uninstalled numpy-1.11.3\n",
      "  Found existing installation: scipy 0.18.1\n",
      "    Uninstalling scipy-0.18.1:\n",
      "      Successfully uninstalled scipy-0.18.1\n",
      "  Running setup.py install for Theano ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
      "\u001b[?25hSuccessfully installed Theano-0.9.0 numpy-1.12.1 scipy-0.19.0\n",
      "Collecting https://github.com/Lasagne/Lasagne/archive/master.zip\n",
      "  Downloading https://github.com/Lasagne/Lasagne/archive/master.zip (224kB)\n",
      "\u001b[K    100% |████████████████████████████████| 225kB 495kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: numpy in /anaconda/lib/python3.6/site-packages (from Lasagne==0.2.dev1)\n",
      "Installing collected packages: Lasagne\n",
      "  Running setup.py install for Lasagne ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25hSuccessfully installed Lasagne-0.2.dev1\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade https://github.com/Theano/Theano/archive/master.zip\n",
    "! pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Архитектура сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn, 128,\n",
    "                                    grad_clipping=100, nonlinearity=lasagne.nonlinearities.tanh, only_return_final=True)\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1, output_size=128)\n",
    "title_nn = lasagne.layers.LSTMLayer(title_nn, 128,\n",
    "                                    grad_clipping=100, nonlinearity=lasagne.nonlinearities.tanh, only_return_final=True)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp, 64)\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_nn, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.concat([descr_nn, title_nn, cat_nn])\n",
    "\n",
    "nn = lasagne.layers.DenseLayer(nn, 512)\n",
    "nn = lasagne.layers.DropoutLayer(nn, p=0.3)\n",
    "nn = lasagne.layers.DenseLayer(nn, 1, nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функция потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction,target_y,delta = 1., log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adamax(loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прогнозирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction,target_y,delta = 1., log_odds=True).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/theano/tensor/basic.py:5130: UserWarning: flatten outdim parameter is deprecated, use ndim instead.\n",
      "  \"flatten outdim parameter is deprecated, use ndim instead.\")\n"
     ]
    }
   ],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Цикл обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "def APatK ( y_true,y_predicted, K =32500):\n",
    "    \"\"\"Calculates AP@k given true Y and predictions (probabilities).\n",
    "    Sorts answers by y_predicted to obtain ranking\"\"\"\n",
    "\n",
    "    sort_by_ypred = np.argsort(-y_predicted)\n",
    "    \n",
    "    y_true = y_true[sort_by_ypred]\n",
    "    y_predicted = y_predicted[sort_by_ypred]\n",
    "    \n",
    "    countRelevants = 0\n",
    "    listOfPrecisions = []\n",
    "    \n",
    "    for i in range(min(K,len(y_true))):\n",
    "        currentk = i + 1.0\n",
    "        if y_true[i] !=0:\n",
    "            countRelevants+=1\n",
    "        precisionAtK = countRelevants / currentk \n",
    "        listOfPrecisions.append(precisionAtK)\n",
    "    return np.sum( listOfPrecisions ) / min(K,len(y_true)) \n",
    "\n",
    "def score(final_accuracy,final_auc,final_apatk):\n",
    "    \n",
    "    print (\"\\nAUC:\")\n",
    "    if final_auc >= 0.99:\n",
    "        print (\"\\tПиши статью. (great)\")\n",
    "    elif final_auc >= 0.97:\n",
    "        print (\"\\tОтличное решение! (good)\")\n",
    "    elif final_auc >= 0.95:\n",
    "        print (\"\\tСойдёт, хотя можно ещё поднажать (ok)\")\n",
    "    elif final_auc >= 0.9:\n",
    "        print (\"\\tНеплохо, но ты можешь лучше! (not ok)\")\n",
    "    elif final_auc > 0.8:\n",
    "        print (\"\\tТы на правильном пути! (not ok)\")\n",
    "    elif final_auc > 0.65:\n",
    "        print (\"\\tДобавь жару! (not ok)\")\n",
    "    else:\n",
    "        print (\"\\tМожет быть, она недоучилась? Ну или слишком маленькая? Или в детстве болела? (not ok)\")\n",
    "    \n",
    "        \n",
    "    print (\"\\nAccuracy:\")\n",
    "    if final_accuracy >= 0.97:\n",
    "        print (\"\\tОчешуенно! (great)\")\n",
    "    elif final_accuracy >= 0.95:\n",
    "        print (\"\\tОтличный результат! (good)\")\n",
    "    elif final_accuracy >= 0.9:\n",
    "        print (\"\\tВсё ок (ok)\")\n",
    "    else:\n",
    "        print (\"Надо бы подтянуть. (not ok)\")\n",
    "\n",
    "    print (\"\\nAverage precision at K:\")\n",
    "    if final_apatk > 0.99:\n",
    "        print (\"\\tЗасабмить на kaggle! (great) \\n\\t Нет, ну честно - выкачай avito_test.tsv, засабмить и скажи, что вышло.\")\n",
    "    elif final_apatk > 0.95:\n",
    "        print (\"\\tОтличный результат (good)\")\n",
    "    elif final_apatk > 0.92:\n",
    "        print (\"\\tВы побили baseline (ok)\")\n",
    "    else:\n",
    "        print (\"\\tНадо бы поднажать (not ok)\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренил на 50 эпохах и остановил процесс, когда получился достаточно хороший скор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train:\n",
      "\tloss: 0.15718284054\n",
      "\tacc: 0.936534653465\n",
      "\tauc: 0.97924985044\n",
      "\tap@k: 0.996609209414\n",
      "Val:\n",
      "\tloss: 0.194513397073\n",
      "\tacc: 0.919405940594\n",
      "\tauc: 0.975087601206\n",
      "\tap@k: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    print (\"Epoch: \" + str(i))\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print (\"Train:\")\n",
    "    print ('\\tloss:',b_loss/b_c)\n",
    "    print ('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print ('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print ('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print (\"Val:\")\n",
    "    print ('\\tloss:',b_loss/b_c)\n",
    "    print ('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print ('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print ('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Финальный подсчет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "\tloss: 0.202780129969\n",
      "\tacc: 0.91724836212\n",
      "\tauc: 0.973360833128\n",
      "\tap@k: 0.993394280157\n",
      "\n",
      "AUC:\n",
      "\tОтличное решение! (good)\n",
      "\n",
      "Accuracy:\n",
      "\tВсё ок (ok)\n",
      "\n",
      "Average precision at K:\n",
      "\tЗасабмить на kaggle! (great) \n",
      "\t Нет, ну честно - выкачай avito_test.tsv, засабмить и скажи, что вышло.\n"
     ]
    }
   ],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print (\"Scores:\")\n",
    "print ('\\tloss:',b_loss/b_c)\n",
    "print ('\\tacc:',final_accuracy)\n",
    "print ('\\tauc:',final_auc)\n",
    "print ('\\tap@k:',final_apatk)\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

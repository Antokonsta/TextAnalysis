{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1,2. Загрузка датасета в Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "MyFile = pd.read_csv(\"/Users/Anton/Desktop/Учеба/Тексты/smsspamcollection/SMSSpamCollection\", \n",
    "                 delimiter=\"\\t\", names=(\"isSpam\", \"smsText\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Spam в колонке isSpam помечается как 1, ham как 0. Создание списка текстов сообщений и списка меток класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,MyFile.isSpam.size):\n",
    "    MyFile.isSpam[i] = int(MyFile.isSpam[i] == \"spam\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [MyFile.smsText[i] for i in range(0,MyFile.smsText.size)]\n",
    "tags = [MyFile.isSpam[i] for i in range(0,MyFile.isSpam.size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Получение из списка текстов матрицу признаков Х"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Получение оценки качества классификации текстов. Полученое значение 0.932640298361"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932640298361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cls = LogisticRegression()\n",
    "res = cross_val_score(cls, X, tags, scoring=\"f1\", cv=10)\n",
    "print (np.mean(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Обучение классификатора на всей выборке и прогнозирование класса на примерах. Из результата видно, что классификатор отнес первые два сообщения к спаму, остальные три к хаму."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.fit(X, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "sample = vectorizer.transform([\"FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! Subscribe6GB\",\n",
    "                               \"FreeMsg: Txt: claim your reward of 3 hours talk time\", \n",
    "                               \"Have you visited the last lecture on physics?\",\n",
    "                               \"Have you visited the last lecture on physics? Just buy this book and you will have all materials! Only 99$\" ,\n",
    "                              \"Only 99$\"])\n",
    "prediction = cls.predict(sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.Оценка качества с добавлением в признаки n-граммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82 0.73 0.93\n"
     ]
    }
   ],
   "source": [
    "vectorizer1 = CountVectorizer(ngram_range=(2,2))\n",
    "X1 = vectorizer1.fit_transform(texts)\n",
    "res1 = cross_val_score(cls, X1, tags, scoring=\"f1\", cv=10)\n",
    "\n",
    "vectorizer2 = CountVectorizer(ngram_range=(3,3))\n",
    "X2 = vectorizer2.fit_transform(texts)\n",
    "res2 = cross_val_score(cls, X2, tags, scoring=\"f1\", cv=10)\n",
    "\n",
    "vectorizer3 = CountVectorizer(ngram_range=(1,3))\n",
    "X3 = vectorizer3.fit_transform(texts)\n",
    "res3 = cross_val_score(cls, X3, tags, scoring=\"f1\", cv=10)\n",
    "\n",
    "print str(round(np.mean(res1),2)) + \" \" + str(round(np.mean(res2),2)) + \" \" + str(round(np.mean(res3),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Повторение эксперимента из пункта 7, но с использованием наивного Байеса. Можно заметить резкое снижение качества, связанное с нехваткой статистики по биграммам и триграммам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65 0.38 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "vectorizer4 = CountVectorizer(ngram_range=(2,2))\n",
    "X4 = vectorizer4.fit_transform(texts)\n",
    "res4 = cross_val_score(nb, X4, tags, scoring=\"f1\", cv=10)\n",
    "\n",
    "vectorizer5 = CountVectorizer(ngram_range=(3,3))\n",
    "X5 = vectorizer5.fit_transform(texts)\n",
    "res5 = cross_val_score(nb, X5, tags, scoring=\"f1\", cv=10)\n",
    "\n",
    "vectorizer6 = CountVectorizer(ngram_range=(1,3))\n",
    "X6 = vectorizer6.fit_transform(texts)\n",
    "res6 = cross_val_score(nb, X6, tags, scoring=\"f1\", cv=10)\n",
    "\n",
    "print str(round(np.mean(res4),2)) + \" \" + str(round(np.mean(res5),2)) + \" \" + str(round(np.mean(res6),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.Использование признаков Tf*idf из TfidfVectorizer на униграммах в логистической регрессии. Качество ухудшилось, 0.85 против 0.93 на CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,1))\n",
    "X_tfidf = tfidf.fit_transform(texts)\n",
    "cls1 = LogisticRegression()\n",
    "\n",
    "res7 = cross_val_score(cls1, X_tfidf, tags, scoring=\"f1\", cv=10)\n",
    "\n",
    "print(round(np.mean(res7),2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.Тестирование различных параметров, классификаторов и способов обработки текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Pipeline c данными параметрами продемонстрировал качество на кросс-валидациях в 0,951"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.951724244087\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_processing_pipeline = Pipeline([\n",
    "        ('Vectorizer', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "        ('Classifier', LogisticRegression(C = 1000))\n",
    "    ])\n",
    "\n",
    "res8 = cross_val_score(text_processing_pipeline,texts, tags, scoring=\"f1\", cv=10)\n",
    "print(np.mean(res8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "б) Использование стохастического градиентного спуска в качестве классификатора в лучшем случае(при данном наборе параметров) продемонстрировало качество примерно равное 0,94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.938020158284\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "reg = SGDClassifier(loss=\"log\", penalty=\"l2\")\n",
    "resss = cross_val_score(reg, X, tags, scoring=\"f1\", cv=10)\n",
    "print (np.mean(resss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в) Использование дисижен три в качестве классификатора показало качество примерно равное 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.882366100781\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "resss1 = cross_val_score(clf, X, tags, scoring=\"f1\", cv=10)\n",
    "print (np.mean(resss1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "г) Фильтрация текстов от стоп-слов не помогла повысить качество, которое только ухудшилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Anton/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "filtered = texts\n",
    "for i in range(0,5572):\n",
    "    stri = \"\"\n",
    "    for w in texts[i].split(\" \"):\n",
    "        if w.lower() not in stopwords.words('english'):\n",
    "            stri = stri + w + \" \"\n",
    "    filtered[i] = stri\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.928275521277\n"
     ]
    }
   ],
   "source": [
    "X_filtered = vectorizer.fit_transform(filtered)\n",
    "\n",
    "cls_filtered = LogisticRegression()\n",
    "res_filtered = cross_val_score(cls_filtered, X_filtered, tags, scoring=\"f1\", cv=10)\n",
    "print (np.mean(res_filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "д) Использование стэмминга увеличило качество модели до 0,9357."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MyFile_stem = pd.read_csv(\"/Users/Anton/Desktop/Учеба/Тексты/smsspamcollection/SMSSpamCollection\", \n",
    "                 delimiter=\"\\t\", names=(\"isSpam\", \"smsText\"),encoding='utf-8')\n",
    "    \n",
    "texts_stem = [MyFile_stem.smsText[i] for i in range(0,MyFile_stem.smsText.size)]\n",
    "\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stem = texts\n",
    "for i in range(0,5572):\n",
    "    stri = \"\"\n",
    "    for w in texts_stem[i].split(\" \"):\n",
    "        stri = stri + ps.stem(w) + \" \"\n",
    "    stem[i] = stri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.935769174389\n"
     ]
    }
   ],
   "source": [
    "X_stem = vectorizer.fit_transform(stem)\n",
    "\n",
    "cls_stem = LogisticRegression()\n",
    "res_stem = cross_val_score(cls_stem, X_stem, tags, scoring=\"f1\", cv=10)\n",
    "print (np.mean(res_stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "е) Использование лемматизации увеличило качество модели до 0,9357."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma = texts\n",
    "for i in range(0,5572):\n",
    "    stri = \"\"\n",
    "    for w in texts[i].split(\" \"):\n",
    "        stri = stri + lemmatizer.lemmatize(w) + \" \"\n",
    "    lemma[i] = stri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.935736895793\n"
     ]
    }
   ],
   "source": [
    "X_lemma = vectorizer.fit_transform(lemma)\n",
    "\n",
    "cls_lemma = LogisticRegression()\n",
    "res_lemma = cross_val_score(cls_lemma, X_lemma, tags, scoring=\"f1\", cv=10)\n",
    "print (np.mean(res_lemma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Наблюдения и выводы из задания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Отсутствие статистики по биграммам и триграммам сказывается на качестве классификатора, особенно страдают нелинейные классификаторы. В то же время при использовании биграмм и триграмм вместе с униграммами в линейном классификаторе, качество существенно не пострадает, т.к. из-за регуляризации данный классификатор не склонен сильно переобучаться на этих признаках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Подбор параметров классификатора способен улучшить качество классификатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Различные классификаторы в большинстве своем демонстрируют различное качество"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Фильтрация текстов от стоп-слов не всегда гарантирует улучшение качества классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
